{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Samples for NLP Beginners\n",
    "\n",
    "### Written by: Robert Thombley, UCSF (July 19th, 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Extraction\n",
    "\n",
    "The pipeline begins with some means of extracting relevant data, and storing it in a data structure that is easy to work with.  In this example, we will pretend that our data lives within a fictious database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import PyODBC # For step 1\n",
    "import re # For step 2\n",
    "import nltk # Python Natural Language Toolkit\n",
    "import spacy # Alternative NLP library that is fast, modern and easier to use\n",
    "import string # Includes some functions and constants useful for working with text data\n",
    "from nltk.corpus import stopwords # May require downloading the nltk corpora to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Conenct to the database we are interested in\n",
    "connection = pyodbc.connect(conn = pyodbc.connect(r'DRIVER={ODBC Driver 11 for SQL Server};'\n",
    "                                                    r'SERVER=testServer.ucsf.edu;'\n",
    "                                                    r'DATABASE=clinical_notes;'\n",
    "                                                    r'UID=user123;'\n",
    "                                                    r'PWD=p@ssword@bC'\n",
    "                                                    ))\n",
    "\n",
    "# Instantiate a cursor object on our database of interest.\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a simple SQL query for extracting a single note from the table CLINICAL_NOTE_TEXT from the\n",
    "# fictitious database clinical_notes\n",
    "\n",
    "sql_string = 'SELECT NOTE_ID, AUTHOR_ID, EDIT_TIMESTAMP_UTC, NOTE_TEXT FROM CLINICAL_NOTE_TEXT where NOTE_ID = 12345'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execute our SQL query\n",
    "query_results_obj = cursor.execute(sql_string)\n",
    "\n",
    "# For each row in the query results object, store the information we care about: note_id, author_id, and note text\n",
    "notes = []\n",
    "for row in query_results_obj:\n",
    "    notes.append(row['note_id'])\n",
    "    notes.append(row['author_id'])\n",
    "    notes.appedn(row['note_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output for note_id = 12345"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[12345, 44442, ‘⌋ History of present illness:\n",
    "44 y/o male w/ h/o EtOH abuse, pancreatitis and nonspecific pain.\n",
    "In [**Name **] pt's vital sign's intially stable, but during interview and while drawing blood, pt went into acute sz and was given Valium 10mg IVx1\n",
    "Social History: Pt is homeless. Smokes 2 packs a day’] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Formatting and Cleaning\n",
    "\n",
    "Here we will introduce some basic data cleaning concepts like regexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove stray unicode mark at the start of the note text\n",
    "notes[2] = re.sub(r’⌋\\s*‘, ‘’, notes[2])\n",
    "\n",
    "# Replace [**Name **] with #name#\n",
    "notes[2] = re.sub(r’\\[\\*\\*Name \\(NI\\) \\*\\*\\]’),‘#name#’, Notes[2])\n",
    "\n",
    "# Decide to remove all headings (e.g.: 'History of present illness: ') from text\n",
    "lines = notes[2].split('\\n')\n",
    "newnote = []\n",
    "heading_re = re.compile(r'.*\\:\\s&*(.*)') # Compile a regex for finding text that precedes a colon and a space\n",
    "\n",
    "for line in lines:    \n",
    "    match_group = heading_re.match(row)\n",
    "    if match_group: \n",
    "        # if the current line matches the heading_re regex pattern, then only add the text after the colon\n",
    "        # to our output\n",
    "        newnote.append(match_group.group(1))    \n",
    "    else:\n",
    "        # otherwise, add the whole row\n",
    "        newnote.append(row)\n",
    "\n",
    "# Join note lines using the newline character and overwrite the old note data\n",
    "notes[2] = ’\\n’.join(newnote)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[12345, 44442, ‘44 y/o male w/ h/o EtOH abuse, pancreatitis and nonspecific pain.\n",
    "In #name# pt's vital sign's intially stable, but during interview and while drawing blood, pt went into acute sz and was given Valium 10mg IVx1\n",
    "Pt is homeless. Smokes 2 packs a day’] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Tokenization Steps\n",
    "After extraction and cleaning, the other steps can vary in their ordering. Here we present some tokenization techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Tokenization/Segmentation is deciptively simple to accomplish using NLTK/SPACY. Unfortunately, there are many assumptions that go into splitting a document into sentences. You may have to iterate between data cleaning and sentence tokenization/segmentation steps until things look like you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using NLTK\n",
    "sentences = nltk.tokenize.sent_tokenize(notes[2])\n",
    "for sent_num, sentence in enumerate(sentences):\n",
    "    print('{} => {}'.format(sent_num, sentence)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0=> ‘44 y/o male w/ h/o EtOH abuse, pancreatitis and nonspecific pain.’\n",
    "1=> ’In #name# pt's vital sign's intially stable, but during interview and while drawing blood, pt went into acute sz and was given Valium 10mg IVx1\n",
    "Pt is homeless.’\n",
    "2=> ‘Smokes 2 packs a day’ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that NLTK's sentence tokenizer chose not to treat a newline as a sentence break.  This makes sense, because often sentences can span multiple lines. In our case, we don't want this.  Our options are: we can try a different sentence tokenizer or we can try to correct the formatting using regexes (ie - iterate back to data cleaning.) Let's just try it using SPACY's sentence tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using SPACY\n",
    "nlp = spacy.load('en_core_web_en') # load the default english language model\n",
    "\n",
    "doc = nlp(notes[2]) # Build the document model\n",
    "\n",
    "#list the segmented sentences\n",
    "for sent_num, sentence in enumerate(doc.sents):\n",
    "    print('{} => {}'.format(sent_num, sentence.text.strip()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0=> ‘44 y/o male w/ h/o EtOH abuse, pancreatitis and nonspecific pain.’\n",
    "1=> ’In #name# pt's vital sign's intially stable, but during interview and while drawing blood, pt went into acute sz and was given Valium 10mg IVx1’\n",
    "2=> 'Pt is homeless.’\n",
    "3=> ‘Smokes 2 packs a day’ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like this worked out how we wanted. For a lot of this, you will have to play around with options to see what works best for your particular use case.\n",
    "\n",
    "Tokenizing our Sentences\n",
    "Most NLP tools assume your document is a collection of sentences, which are themselves collections of tokens. These tokens can be unigrams, bi-grams, tri-grams or many other token types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unigram tokenization using NLTK\n",
    "unigrams = nltk.tokenize.word_tokenize(notes[2])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "['44', 'y/o', 'male', 'w/', 'h/o', 'EtOH', 'abuse', ',', \n",
    "'pancreatitis', 'and', 'nonspecific', 'pain', '.',\n",
    "'In', '#', 'name', '#', 'pt', \"'s\", 'vital', 'sign', \"'s\", 'intially', 'stable'\n",
    "...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Steps, as needed:\n",
    "\n",
    "Complexity Reduction is used to reduce repetition and redundancy in your text data. We want to consider 'pt' and 'patient' to be the same token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a set of stop words (commonly used words that carry little information)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Build a set of punctuation characters (for our purposes, we've decided punctuation isn't very important)\n",
    "punct = set([s for s in string.punctuation])\n",
    "\n",
    "# Build a set of common clinical abbreviations\n",
    "med_abbrev = {'y/o': 'year old', 'w/': 'with', 'h/o': 'history of', 'EtOH': 'alcohol', 'pt': 'patient'}\n",
    "\n",
    "tok_reduced = []\n",
    "for tok in unigrams:\n",
    "    if tok not in stop_words and tok not in punct:\n",
    "        if tok in med_abbrev.keys():\n",
    "            tok = med_abbrev[tok] # expand the abbreviation\n",
    "        tok_reduced.append[tok]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tok_reduced =\n",
    "\n",
    "['year old', 'male', 'with', 'history of', 'alcohol', 'abuse', 'pancreatitis', 'nonspecific', 'pain', 'In', 'name', 'patient', \"'s\", 'vital', 'sign', \"'s\", 'intially', 'stable']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS Tagging is used to tag the part of speech of a particular word, usually based on a statistcal model of word use (ie- what's the most likely part of speech for this word, given what we've seen in a huge amount of training data). Getting POS tagging to work well, especially for jargon-heavy data like clinical text is a challenge, because most of the statistical models that are freely available haven't encountered domain specific semantics. The basic NTLK POS tagging works ok, but domain specific taggers exist (like the MedPost SKR Tagger used by MetaMap: https://metamap.nlm.nih.gov/MedPostSKRTagger.shtml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tok = nltk.pos_tag(tok_reduced)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pos_tok =\n",
    "\n",
    "[('years old', 'ADV'), \n",
    "('male', 'ADJ'), \n",
    "('with', 'NOUN'), \n",
    "('history of', 'NOUN'), \n",
    "('alcohol', 'NOUN'), \n",
    "('abuse', 'NOUN'), \n",
    "('pancreatitis', 'NOUN')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
