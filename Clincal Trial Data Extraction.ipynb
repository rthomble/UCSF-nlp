{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Raw Data for Use in NLP Pipeline\n",
    "\n",
    "### Written By: Robert Thombley, UCSF (7/19/2018)\n",
    "Before we begin our classification task, we need to gather the raw data and put it into a form that's easily  accessible, in the proper format and conducive to working in Python. All of the raw data for this project lives at https://clinicaltrials.ucsf.edu/browse/.  We will be using the BeautifulSoup Python library to scrape this website and harvest all of the potentially informative text from each clinical trial's description.\n",
    "    \n",
    "This website is formatted such that most clinical trials are categorized by major body system or condition, then subcategorized into sub-conditions.  The conditions are listed as headings, with each of the subconditions listed as a link underneath that heading. When we follow the subcondition link, we end up with a list of all clinical trials categorized into that condition/sub-condition bucket.  Each of the listed clinical trials here is a link to the clinical trial description page, which holds the text that we are most interested in.\n",
    "    \n",
    "As we are building our data structure without a great sense of the features we are interested in modeling, we will have to make some assumptions about the schema. Recognize that we may make a mistake here and leave out an important variable, but we'll have to cross that bridge when we get there.  For now, it seems to me that these are the interesting data points:\n",
    "* Condition name/heading: This is the bucket name into which we will try to categorize the \"other\" trials. It is a critical value and we should be able to easily subset our data structure using this key (Hint: This means we should probably be using a dictionary/hash table as our main data structure)\n",
    "* Sub condition name: We won't use this as a categorization label, but it may be a valuable piece of information.\n",
    "* Sub condition link: It might be useful to retain this, but it isn't super critical to keep around.\n",
    "* Number of trials: Could be useful for to get a sense of relative importance of sub-conditions, but it's probably not that useful.\n",
    "* Clinical Trial name: This is the actual title of the trial and is an important piece of text data.\n",
    "* Clinical Trial description page link: This will be important to retain so that we can easily add more data later\n",
    "* Clinical Trial study text: This is the actual text from the study page. I made some assumptions about what is useful and what is not, but this is our payload and the content we actually care about.\n",
    "\n",
    "So, given the fact that the data that's available exists as heteogeneous types (ie - objects/chars/etc) and the requirement that we need to be able to easily subset by the condition heading (ie - 'cancer', 'eyes and vision', etc), this points to using a dictionary of a list of objects as our data structure, something like:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data[condition_name] = [condition_name,\n",
    "                        sub_condition_name, \n",
    "                        study_name, \n",
    "                        study_link,\n",
    "                        [study_data_sentence1,\n",
    "                         study_data_sentence2,\n",
    "                         ...\n",
    "                         study_data_sentenceN]\n",
    "                        ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! Let's build it.\n",
    "Start by importing the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests # Performs HTTP requests for returning webpage data\n",
    "from bs4 import BeautifulSoup # Web Scraping library\n",
    "import json # Allows us to save python objects in text format\n",
    "import re # Regular expressions library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the file location on disk for storing the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file = '/path/to/data_repository/clinical_trial_data.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work backwards a little bit.  Let's create a function that we can call with the clinical trial description page link.  This will allow us to call this function each time we want to extract data from the study page, simplifying our scraping code.\n",
    "\n",
    "Given an example trial description page, https://clinicaltrials.ucsf.edu/trial/NCT02548598, I only want to extract the Description and Eligibility sections, since those seem to hold the information that is actually condition-related.\n",
    "\n",
    "Each of the sections (title, Description, Eligibility ,etc) is <DIV> that is a member of the 'show-jargon-defitions' CSS class.  This function just finds all instances of this class in the beautiful soup object and extracts the text within the 2nd and 3rd (indexes 1 and 2) sections ('Description' and 'Eligibility', respectively). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTextFromURL(url):\n",
    "    page_obj = BeautifulSoup(requests.get(url).content, 'lxml')\n",
    "    sections = page_obj.findAll('div','show-jargon-definitions')\n",
    "    \n",
    "    # Grab all text, storing each line as a new position in the text_out list.\n",
    "    # We skip any lines that are empty.\n",
    "    text_out = [line for line in sections[1].findAll(text=True) if line != ' ']   \n",
    "    text_out.extend([line for line in sections[2].findAll(text=True) if line != ' '])\n",
    "    return(text_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start back at the main page and use the requests library to get all of the HTML from the main page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_url = \"https://clinicaltrials.ucsf.edu/browse/\"\n",
    "urldat = requests.get(base_url) #\n",
    "page_content = urldat.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Convert the HTML data into a beautiful soup object and identify all of the headings that are instances of the 'browse-condition-cluster--block' CSS class.  These are our conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page_content, \"lxml\")\n",
    "headings = soup.find_all(\"div\", \"browse-condition-cluster--block\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section builds our dataset (Warning: This takes a long time to run (> 10min) and creates a ~25Mb dataset, so I've added 3 \"ejection commands\" to keep it from running too long.  Remove these to create the full dataset.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acute Lymphoblastic Leukemia => ../acute-lymphoblastic-leukemia\n",
      "1: A Multi-Center Study Evaluating KTE-C19 in Pediatric and Adolescent Subjects With Relapsed/Refractory B-precursor Acute Lymphoblastic Leukemia\n",
      "2: A Multicenter Access and Distribution Protocol for Unlicensed Cryopreserved Cord Blood Units (CBUs)\n",
      "Acute Coronary Syndrome => ../acute-coronary-syndrome\n",
      "1: Tailored Antiplatelet Therapy Following PCI\n",
      "Acute Myocardial Infarction => ../acute-myocardial-infarction\n",
      "1: Prospective ARNI vs ACE Inhibitor Trial to DetermIne Superiority in Reducing Heart Failure Events After MI\n",
      "Dislocation of Shoulder Region => ../dislocation-of-shoulder-region\n",
      "1: MOON Shoulder Instability-Cohort of Patients Undergoing Operative Treatment.\n",
      "Fibrodysplasia Ossificans Progressiva => ../fibrodysplasia-ossificans-progressiva\n",
      "1: An Efficacy and Safety Study of Palovarotene for the Treatment of FOP\n",
      "2: In-Home Evaluation of Episodic Administration of Palovarotene in Fibrodysplasia Ossificans Progressiva (FOP) Subjects\n"
     ]
    }
   ],
   "source": [
    "data = {};\n",
    "for heading in headings: # Iterate over all the headings on the browse page\n",
    "    # In order to be used as a key, we need to remove commas, replace spaces with underscores, \n",
    "    # and convert the text to all lower case.\n",
    "    \n",
    "    key = heading.h2.contents[0].strip().lower().replace(',','').replace(' ','_')  \n",
    "    data[key] = [] \n",
    "    \n",
    "    # Beautiful soup sometimes parses pages in a tricky way. Often you will have to experiment to see \n",
    "    # what works best for accessing the data you want. Here we find all of the <span> tags that are \n",
    "    # children of the current <div class='browse-condition-cluster--block'> (ie -heading)\n",
    "    \n",
    "    for sub_condition_span in heading.div.findAll(\"span\"):\n",
    "        if sub_condition_span.a: # We only care about the span if it contains a link.\n",
    "            # Extract the name and the link for each subcondition.\n",
    "            sub_condition_name = sub_condition_span.findAll(\"span\", \"browse-condition-cluster--condition-name\")[0].string\n",
    "            sub_link = sub_condition_span.a.attrs['href']\n",
    "            \n",
    "            # Print sub_condition_name and link to keep track of where we are\n",
    "            print(\"{} => {}\".format(sub_condition_name, sub_link))\n",
    "            \n",
    "            # Load the sub-condition page into a Beautiful Soup object\n",
    "            sub_page_dat = BeautifulSoup(requests.get(base_url + sub_link).content, 'lxml')\n",
    "            \n",
    "            # Trials are displayed as an unordered list with the CSS class 'list-unstyled'\n",
    "            # However, there are some <li> tags we don't care about, so we \n",
    "            # need to filter by list items that include the text 'trials-list' somewhere in them.\n",
    "            # We need to use a regex to complete this filter.\n",
    "            \n",
    "            trial_list = sub_page_dat.find(\"ul\", \"list-unstyled\").findAll('li', re.compile(\"trials-list\"))\n",
    "            \n",
    "            # For every trial we identify, extract relevant information and call the getTextFromUrl() function\n",
    "            for trial_num, trial in enumerate(trial_list):\n",
    "                study_name = trial.h2.a.string\n",
    "                study_link = trial.h2.a.attrs['href']\n",
    "                study_data = getTextFromURL(study_link)\n",
    "                print(\"{}: {}\".format(trial_num+1, study_name))\n",
    "                \n",
    "                # Store all of the data into our data structure\n",
    "                data[key].append([key, sub_condition_name, study_name, study_link, study_data])\n",
    "                \n",
    "                # EJECTION command #1: exit the loop after 1 iteration so that this runs quickly.\n",
    "                # Delete this if you want to generate the full dataset\n",
    "                if trial_num > 0:\n",
    "                    break\n",
    "                    \n",
    "            # EJECTION command #2\n",
    "            if len(data[key]) > 1:\n",
    "                break\n",
    "\n",
    "    # EJECTION command #3\n",
    "    if len(data.keys()) > 2: \n",
    "        break\n",
    "# Write the data file to disk. I like to serialize objects (ie- write objects to disk in a way that makes them easier\n",
    "# to load back into memory) using json. You could try this with pickle or some other serialization library.\n",
    "\n",
    "with open(data_file, 'w') as fp:\n",
    "    json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cancer',\n",
       " 'Acute Lymphoblastic Leukemia',\n",
       " 'A Multicenter Access and Distribution Protocol for Unlicensed Cryopreserved Cord Blood Units (CBUs)',\n",
       " 'https://clinicaltrials.ucsf.edu/trial/NCT01351545',\n",
       " ['Summary',\n",
       "  'This study is an access and distribution protocol for unlicensed cryopreserved cord blood units (CBUs) in pediatric and adult patients with hematologic malignancies and other indications.',\n",
       "  'Official Title',\n",
       "  'A Multicenter Access and Distribution Protocol for Unlicensed Cryopreserved Cord Blood Units (CBUs) for Transplantation in Pediatric and Adult Patients With Hematologic Malignancies and Other Indications',\n",
       "  'Details',\n",
       "  'Principal Investigators:',\n",
       "  'The principal investigators (PIs) will be transplant physicians at all participating U.S. transplant centers.',\n",
       "  'Study Design:',\n",
       "  'This study is an access and distribution protocol for unlicensed cryopreserved cord blood units (CBUs) in pediatric and adult patients with hematologic malignancies and other indications.',\n",
       "  'Primary Objective:',\n",
       "  'The primary objective of this study is to examine the incidence of neutrophil recovery of ≥500/mm3 after cord blood transplantation in a multi-institution setting using CBUs that are not Food and Drug Administration (FDA) licensed.',\n",
       "  'Secondary Objectives:',\n",
       "  'In patients receiving a non-licensed CBU:',\n",
       "  'Assess incidence of transmission of infection',\n",
       "  'Assess incidence of serious infusion reaction',\n",
       "  'Determine 1 year overall survival after cord blood transplantation',\n",
       "  'Assess cumulative incidence of acute graft vs. host disease (GVHD) grades II to IV and grades III to IV',\n",
       "  'Assess cumulative incidence of chronic GVHD',\n",
       "  'Determine platelet engraftment of >20,000 mcL and >50,000 mcL',\n",
       "  'Keywords',\n",
       "  'Hematologic Malignancies',\n",
       "  'Inherited Disorders of Metabolism',\n",
       "  'Inherited Abnormalities of Platelets',\n",
       "  'Histiocytic Disorders',\n",
       "  'Acute Myelogenous Leukemia (AML or ANLL)',\n",
       "  'Acute Lymphoblastic Leukemia (ALL)',\n",
       "  'Other Acute Leukemia',\n",
       "  'Chronic Myelogenous Leukemia (CML)',\n",
       "  'Myelodysplastic (MDS) / Myeloproliferative (MPN) Diseases',\n",
       "  'Other Leukemia',\n",
       "  'Hodgkin Lymphoma',\n",
       "  'Non-hodgkin Lymphoma',\n",
       "  'Multiple Myeloma/ Plasma Cell Disorder (PCD)',\n",
       "  'Inherited Abnormalities of Erythrocyte Differentiation or Function',\n",
       "  'Disorders of the Immune System',\n",
       "  'Automimmune Diseases',\n",
       "  'Severe Aplastic Anemia',\n",
       "  'Lymphoma',\n",
       "  'Disease',\n",
       "  'Leukemia',\n",
       "  'Multiple Myeloma',\n",
       "  'Neoplasms',\n",
       "  'Lymphoma, Non-Hodgkin',\n",
       "  'Precursor Cell Lymphoblastic Leukemia-Lymphoma',\n",
       "  'Leukemia, Lymphoid',\n",
       "  'Leukemia, Myeloid',\n",
       "  'Hodgkin Disease',\n",
       "  'Leukemia, Myelogenous, Chronic, BCR-ABL Positive',\n",
       "  'Congenital Abnormalities',\n",
       "  'Leukemia, Myeloid, Acute',\n",
       "  'Anemia, Aplastic',\n",
       "  'A multicenter access and distribution protocol for unlicensed cryopreserved cord blood units (CBUs)',\n",
       "  'Unlicensed CBU',\n",
       "  'Eligibility',\n",
       "  ' You can join if… ',\n",
       "  ' Disorders affecting the hematopoietic system that are inherited, acquired, or result from myeloablative treatment',\n",
       "  ' Signed informed consent (and signed assent, if applicable) obtained prior to study enrollment',\n",
       "  ' Pediatric and adult patients of any age',\n",
       "  \" You CAN'T join if... \",\n",
       "  ' Patients who are receiving only licensed CBUs',\n",
       "  ' Cord blood transplant recipients at international transplant centers',\n",
       "  ' Patients who are enrolled on another IND protocol to access the unlicensed CBU(s)',\n",
       "  ' Patients whose selected unlicensed CBU(s) will be more than minimally manipulated']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cancer'][1] # Demo the format for the 2nd trial in the 'other' category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
