{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Introduction to Clinical Text Processing using Spacy\n",
    "\n",
    "### Written by: Robert Thombley, UCSF (July 19th, 2018)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very brief example of how to use Spacy for processing text. Unlike NLTK which requires a lot of hands on plumbing to connect all of the NLP pieces, Spacy does most of the work behind the scenes and simplifies a lot of the process.  The advantage is that this gives you the ability to code quickly and, relatively, simply allowing you to, best case, solve your problem sooner or, worst case, quickly understand the complicated parts of your task.\n",
    "\n",
    "What follows is only a tiny fraction of what Spacy is capable of. For more information, check out: https://spacy.io/usage/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Local directory filled with (in this case, de-identified fake clinical notes from the web), organized 1 note per file\n",
    "notes_dir = '/PATH/TO/NLP/CLINICAL_DATA' \n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "note_toks = []\n",
    "for filename in os.listdir(notes_dir):\n",
    "    with open(os.path.join(notes_dir, filename), 'r') as f:\n",
    "        # Remove all of the \\n (newline) characters \n",
    "        list_of_lines = [x.strip() for x in f.readlines() if x != '\\n']\n",
    "        note_txt = '. '.join(list_of_lines)\n",
    "        doc = nlp(note_txt)\n",
    "        toks = []\n",
    "        for token in doc:\n",
    "            # The beauty of the Spacy approach is that much of the validation/NLP has been done for you behind the scenes.\n",
    "            # If we want to remove problematic, or uninsteresting parts of speech, that tagging has already been \n",
    "            # for you and is stored, ready to go.  So I can just say: well, I don't want to see any proper nouns,\n",
    "            # any numbers, punctuations or symbols in my output tokens and it's a one line operation:\n",
    "            \n",
    "            if token.pos_ not in ('PROPN', 'NUM', 'PUNCT', 'SYM'):\n",
    "                # Lets say, we also only want to store nouns, because that feels like an appropriate decision.\n",
    "                if token.pos_ == 'NOUN': \n",
    "                    toks.append(token.text.lower())\n",
    "        note_toks.append(toks)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['history',\n",
       "  'illness',\n",
       "  'gentleman',\n",
       "  'hypertension',\n",
       "  'hypercholesterolemia',\n",
       "  'pt',\n",
       "  'reports',\n",
       "  'trouble',\n",
       "  'medications',\n",
       "  'pain',\n",
       "  'mo',\n",
       "  'side',\n",
       "  'day',\n",
       "  'event',\n",
       "  'radiation',\n",
       "  'legs',\n",
       "  'fevers',\n",
       "  'night',\n",
       "  'sweats',\n",
       "  'weight',\n",
       "  'loss',\n",
       "  'bowel',\n",
       "  'bladder',\n",
       "  'problems',\n",
       "  'note',\n",
       "  'radiation',\n",
       "  'area',\n",
       "  'years',\n",
       "  'treatment',\n",
       "  'skin',\n",
       "  'cancer',\n",
       "  'exercise',\n",
       "  'history',\n",
       "  'note',\n",
       "  'review',\n",
       "  'cardiovascular',\n",
       "  'chest',\n",
       "  'pain',\n",
       "  'palpitations',\n",
       "  'orthopnea',\n",
       "  'edema',\n",
       "  'syncope',\n",
       "  'pulmonary',\n",
       "  'shortness',\n",
       "  'breath',\n",
       "  'cough',\n",
       "  'pain',\n",
       "  'changes',\n",
       "  'bowel',\n",
       "  'habits',\n",
       "  'physical',\n",
       "  'signs',\n",
       "  'weight',\n",
       "  'pounds',\n",
       "  'temp',\n",
       "  'heent',\n",
       "  'pink',\n",
       "  'lungs',\n",
       "  'heart',\n",
       "  'rate',\n",
       "  'rhythm',\n",
       "  'murmur',\n",
       "  'extremities',\n",
       "  'edema',\n",
       "  'neurologic',\n",
       "  'sensation',\n",
       "  'dtrs',\n",
       "  'intact',\n",
       "  'musculoskeletal',\n",
       "  'tenderness',\n",
       "  'spine',\n",
       "  'leg',\n",
       "  'hypertension',\n",
       "  'control',\n",
       "  'enalapril',\n",
       "  'day',\n",
       "  'day',\n",
       "  'diet',\n",
       "  'exercise',\n",
       "  'changes',\n",
       "  'potassium',\n",
       "  'today',\n",
       "  'day',\n",
       "  'lipids',\n",
       "  'today',\n",
       "  'pain',\n",
       "  'features',\n",
       "  'history',\n",
       "  'radiation',\n",
       "  'exposure',\n",
       "  'x',\n",
       "  'ray',\n",
       "  'calcium',\n",
       "  'rate',\n",
       "  'psa',\n",
       "  'visit',\n",
       "  'health',\n",
       "  'maintenance',\n",
       "  'fobts',\n",
       "  'return',\n",
       "  'clinic',\n",
       "  'months',\n",
       "  'pain'],\n",
       " ['history',\n",
       "  'history',\n",
       "  'r',\n",
       "  'breast',\n",
       "  'caner',\n",
       "  'mastectomy',\n",
       "  'cocaine',\n",
       "  'use',\n",
       "  'tobacco',\n",
       "  'smoker',\n",
       "  'who',\n",
       "  'cough',\n",
       "  'patient',\n",
       "  'cough',\n",
       "  'times',\n",
       "  'day',\n",
       "  'hemoptysis',\n",
       "  'shortness',\n",
       "  'breath',\n",
       "  'chest',\n",
       "  'pain',\n",
       "  'triggers',\n",
       "  'drinking',\n",
       "  'water',\n",
       "  'inhalers',\n",
       "  'guafenesin',\n",
       "  'cough',\n",
       "  'awakenings',\n",
       "  'hospitalizations',\n",
       "  'abx',\n",
       "  'steroids',\n",
       "  'signs',\n",
       "  'weight',\n",
       "  'pounds',\n",
       "  'temp'],\n",
       " ['nursing',\n",
       "  'o',\n",
       "  'history',\n",
       "  'refer',\n",
       "  'chart',\n",
       "  'details',\n",
       "  'c',\n",
       "  'o',\n",
       "  'polyuria',\n",
       "  'weeks',\n",
       "  'r',\n",
       "  'weakness',\n",
       "  'glucose',\n",
       "  'pt',\n",
       "  'insulin',\n",
       "  'gtt',\n",
       "  'head',\n",
       "  'management',\n",
       "  'hyperglycemia',\n",
       "  'assessment',\n",
       "  'admission',\n",
       "  'pt',\n",
       "  'cath',\n",
       "  'access',\n",
       "  'pt',\n",
       "  'insulin',\n",
       "  'gtt',\n",
       "  'control',\n",
       "  'fluid',\n",
       "  'mult',\n",
       "  'attempts',\n",
       "  'bedside',\n",
       "  'placement',\n",
       "  'meds',\n",
       "  'pt',\n",
       "  'risk',\n",
       "  'aspiration',\n",
       "  'plan',\n",
       "  'resp',\n",
       "  'status',\n",
       "  'placement',\n",
       "  'use',\n",
       "  'volume',\n",
       "  'repletion',\n",
       "  'cycle',\n",
       "  'enzymes',\n",
       "  'insulin',\n",
       "  'gtt',\n",
       "  'control',\n",
       "  'administer',\n",
       "  'evening',\n",
       "  'meds',\n",
       "  'pt',\n",
       "  'update',\n",
       "  'family']]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "note_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Can also use Spacy to do some (many) more advanced things.\n",
    "# You can create noun chunks which uses the part of speech tagging to identify noun phrases or\n",
    "# noun \"chunks\", which are basically parts of phrases that belong together.\n",
    "note_chunks = []\n",
    "for filename in os.listdir(notes_dir):\n",
    "    with open(os.path.join(notes_dir, filename), 'r') as f:\n",
    "        # Remove all of the \\n (newline) characters \n",
    "        list_of_lines = [x.strip() for x in f.readlines() if x != '\\n']\n",
    "        note_txt = '. '.join(list_of_lines)\n",
    "        doc = nlp(note_txt)\n",
    "        chunks = []\n",
    "        # Build a list of noun chunks\n",
    "        for chunk in list(doc.noun_chunks):\n",
    "            chunks.append(chunk)\n",
    "        note_chunks.append(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Phillip D. Smith,\n",
       "  HISTORY,\n",
       "  PRESENT ILLNESS,\n",
       "  Mr. Smith,\n",
       "  a 66-year-old gentleman,\n",
       "  hypertension,\n",
       "  hypercholesterolemia,\n",
       "  pt reports,\n",
       "  he,\n",
       "  He,\n",
       "  any trouble,\n",
       "  his medications,\n",
       "  His bp,\n",
       "  He,\n",
       "  low back pain,\n",
       "  the past 6 mo,\n",
       "  It,\n",
       "  the right side,\n",
       "  the day,\n",
       "  No known initial precipitating event,\n",
       "  He,\n",
       "  any radiation,\n",
       "  his legs,\n",
       "  any fevers,\n",
       "  night sweats,\n",
       "  weight loss,\n",
       "  bowel or bladder problems,\n",
       "  note,\n",
       "  he,\n",
       "  significant radiation,\n",
       "  this area,\n",
       "  treatment,\n",
       "  skin cancer,\n",
       "  He,\n",
       "  much exercise,\n",
       "  PMH,\n",
       "  MEDS,\n",
       "  ALLERGIES,\n",
       "  SOCIAL HISTORY,\n",
       "  July 2005 note,\n",
       "  REVIEW,\n",
       "  SYSTEMS,\n",
       "  CARDIOVASCULAR,\n",
       "  no chest pain,\n",
       "  palpitations,\n",
       "  PND,\n",
       "  orthopnea,\n",
       "  edema,\n",
       "  syncope,\n",
       "  PULMONARY,\n",
       "  no shortness,\n",
       "  breath,\n",
       "  cough,\n",
       "  GI,\n",
       "  no abdominal pain,\n",
       "  changes,\n",
       "  bowel habits,\n",
       "  PHYSICAL EXAMINATION,\n",
       "  VITAL SIGNS,\n",
       "  210.7 pounds,\n",
       "  temp,\n",
       "  HEENT,\n",
       "  Conjunctivae,\n",
       "  Sclerae,\n",
       "  NECK,\n",
       "  LUNGS,\n",
       "  HEART,\n",
       "  Regular rate,\n",
       "  rhythm,\n",
       "  murmur,\n",
       "  EXTREMITIES,\n",
       "  No edema,\n",
       "  NEUROLOGIC,\n",
       "  Strength,\n",
       "  sensation,\n",
       "  DTRs,\n",
       "  MUSCULOSKELETAL,\n",
       "  He,\n",
       "  no tenderness,\n",
       "  his spine,\n",
       "  Straight leg,\n",
       "  ASSESSMENT,\n",
       "  PLAN,\n",
       "  Hypertension,\n",
       "  only fair control,\n",
       "  We,\n",
       "  his enalapril,\n",
       "  diet and exercise changes,\n",
       "  potassium,\n",
       "  Hypercholesterolemia,\n",
       "  Lipitor,\n",
       "  lipids,\n",
       "  ALT,\n",
       "  Back pain,\n",
       "  features,\n",
       "  his history,\n",
       "  radiation exposure,\n",
       "  an x-ray,\n",
       "  CBC,\n",
       "  calcium,\n",
       "  sed rate,\n",
       "  PSA,\n",
       "  He,\n",
       "  a visit,\n",
       "  Physical Therapy,\n",
       "  he,\n",
       "  He,\n",
       "  Health maintenance,\n",
       "  more FOBTs,\n",
       "  Return,\n",
       "  clinic,\n",
       "  6 months,\n",
       "  He,\n",
       "  us,\n",
       "  back pain,\n",
       "  Mary Student,\n",
       "  MS3,\n",
       "  Joe Doctor,\n",
       "  MD],\n",
       " [Lisa M Smith,\n",
       "  HISTORY,\n",
       "  PRESENT ILLNESS,\n",
       "  Patient,\n",
       "  a past medical history,\n",
       "  well controlled HIV,\n",
       "  recent CD4,\n",
       "  R breast caner s/p partial mastectomy,\n",
       "  cocaine use,\n",
       "  active tobacco smoker,\n",
       "  who,\n",
       "  chronic cough,\n",
       "  Patient,\n",
       "  a chronic cough,\n",
       "  all times,\n",
       "  the day,\n",
       "  no hemoptysis,\n",
       "  shortness,\n",
       "  breath,\n",
       "  chest pain,\n",
       "  No identified triggers,\n",
       "  Drinking water,\n",
       "  albuterol inhalers,\n",
       "  guafenesin/dextromethorphan,\n",
       "  cough,\n",
       "  No nighttime awakenings,\n",
       "  no recent hospitalizations,\n",
       "  abx,\n",
       "  systemic steroids,\n",
       "  VITAL SIGNS,\n",
       "  210.7 pounds,\n",
       "  temp],\n",
       " [Nursing Progress,\n",
       "  Problem,\n",
       "  y/o M w/ extensive past cardiac history,\n",
       "  (refer,\n",
       "  chart,\n",
       "  details,\n",
       "  EW c/o polyuria,\n",
       "  3 weeks,\n",
       "  slight R,\n",
       "  weakness,\n",
       "  Serum,\n",
       "  pt,\n",
       "  non ketotic insulin gtt,\n",
       "  head,\n",
       "  M/SICU,\n",
       "  management,\n",
       "  hyperglycemia,\n",
       "  admission,\n",
       "  Foley cath,\n",
       "  central access,\n",
       "  pt,\n",
       "  cool/diaphoretic and lethargic w/ FSBS,\n",
       "  Action Foley,\n",
       "  insulin gtt,\n",
       "  glycemic control,\n",
       "  increased fluid,\n",
       "  100cc/hr mult attempts,\n",
       "  PICC,\n",
       "  bedside,\n",
       "  placed L IJ TLCL,\n",
       "  CXR,\n",
       "  placement,\n",
       "  No AM PO meds,\n",
       "  pt,\n",
       "  aspiration,\n",
       "  Plan Cont,\n",
       "  heme/resp status,\n",
       "  placement,\n",
       "  L IJ,\n",
       "  use,\n",
       "  volume repletion,\n",
       "  Cycle enzymes,\n",
       "  insulin gtt,\n",
       "  tight gylcemic control,\n",
       "  Administer all evening PO meds,\n",
       "  pt,\n",
       "  Update family,\n",
       "  POC,\n",
       "  it]]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "note_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Nursing Progress,\n",
       " Problem,\n",
       " y/o M w/ extensive past cardiac history,\n",
       " (refer,\n",
       " chart,\n",
       " details,\n",
       " EW c/o polyuria,\n",
       " 3 weeks,\n",
       " slight R,\n",
       " weakness,\n",
       " Serum,\n",
       " pt,\n",
       " non ketotic insulin gtt,\n",
       " head,\n",
       " M/SICU,\n",
       " management,\n",
       " hyperglycemia,\n",
       " admission,\n",
       " Foley cath,\n",
       " central access,\n",
       " pt,\n",
       " cool/diaphoretic and lethargic w/ FSBS,\n",
       " Action Foley,\n",
       " insulin gtt,\n",
       " glycemic control,\n",
       " increased fluid,\n",
       " 100cc/hr mult attempts,\n",
       " PICC,\n",
       " bedside,\n",
       " placed L IJ TLCL,\n",
       " CXR,\n",
       " placement,\n",
       " No AM PO meds,\n",
       " pt,\n",
       " aspiration,\n",
       " Plan Cont,\n",
       " heme/resp status,\n",
       " placement,\n",
       " L IJ,\n",
       " use,\n",
       " volume repletion,\n",
       " Cycle enzymes,\n",
       " insulin gtt,\n",
       " tight gylcemic control,\n",
       " Administer all evening PO meds,\n",
       " pt,\n",
       " Update family,\n",
       " POC,\n",
       " it]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's demonstrate a very simple way to use Spacy to create a bag of words TF-IDF matrix for use in modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simple_toks = []\n",
    "sents = ['This is a sample document.',\n",
    "        'This document is written by Robert.',\n",
    "         'Robert is a document creator.']\n",
    "for sent in sents:\n",
    "    doc = nlp(sent)\n",
    "    toks = []\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ('PUNCT', 'SYM'):\n",
    "            # Ignore any punctuation.\n",
    "            toks.append(token.text.lower())\n",
    "    simple_toks.append(toks)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is', 'a', 'sample', 'document'],\n",
       " ['this', 'document', 'is', 'written', 'by', 'robert'],\n",
       " ['robert', 'is', 'a', 'document', 'creator']]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dummy_fun(doc):\n",
    "    # TFidfVectorizer expects to do its own tokenizing and pre-processing,\n",
    "    # but you can tell it to use a custom tokenizer/pre-preocessor.  What this\n",
    "    # function does is returns whatever it put in, allowing us to vectorize, \n",
    "    # token for token, exactly the tokens we input. No further processing is needed.\n",
    "    return doc\n",
    "\n",
    "# Instantiate the TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'by': 1,\n",
       " 'creator': 2,\n",
       " 'document': 3,\n",
       " 'is': 4,\n",
       " 'robert': 5,\n",
       " 'sample': 6,\n",
       " 'this': 7,\n",
       " 'written': 8}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we'll build our vocabulary\n",
    "tfidf.fit(simple_toks)\n",
    "tfidf.vocabulary_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that this is an alphabetized list of all the unique tokens that appear in our corpus.\n",
    "Next, we'll "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.28768207 1.69314718 1.69314718 1.         1.         1.28768207\n",
      " 1.69314718 1.28768207 1.69314718]\n"
     ]
    }
   ],
   "source": [
    "# Now we can print out the inverse document frequencies (ie - 1 over the proportion of documents did these words appear in)\n",
    "print(tfidf.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally, we compute the TF-IDF weight for each feature and document.\n",
    "tf = []\n",
    "for simp_tok in simple_toks:\n",
    "    tf.append((tfidf.transform([simp_tok]).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.45014501, 0.        , 0.        , 0.34957775, 0.34957775,\n",
       "         0.        , 0.59188659, 0.45014501, 0.        ]]),\n",
       " array([[0.        , 0.50935267, 0.        , 0.30083189, 0.30083189,\n",
       "         0.38737583, 0.        , 0.38737583, 0.50935267]]),\n",
       " array([[0.45014501, 0.        , 0.59188659, 0.34957775, 0.34957775,\n",
       "         0.45014501, 0.        , 0.        , 0.        ]])]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
